from openai import OpenAI
from fastapi import FastAPI, File, Form, UploadFile
import uvicorn
import pandas as pd
from io import BytesIO
import os
import re

# Configure DeepSeek
os.environ['DEEPSEEK_API_KEY'] = 'sk-093c3391ec134c55b76decd5225f703a'  # Set your DeepSeek API key here

client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1"  # DeepSeek API endpoint
)

app = FastAPI()
regex_pattern = r"(.?)'''\s([\s\S]?)\s'''(.*)"

@app.post("/generate-prompt")
async def generate_prompt(
    file1: UploadFile = File(...),
    file2: UploadFile = File(...),
    file3: UploadFile = File(...),
):
    print("here")
    # Read CSV files and get first 5 rows
    df1 = pd.read_csv(BytesIO(await file1.read()))
    data1 = df1.head(5).to_csv(index=False)
    
    df2 = pd.read_csv(BytesIO(await file2.read()))
    data2 = df2.head(5).to_csv(index=False)
    
    destination_df = pd.read_csv(BytesIO(await file3.read()))
    destination_columns = ", ".join(destination_df.columns)

    prompt = f"""
    You are AI expert, you are tasked to design an algorithm to recognize the data and its meaning. You are given 5 rows and your job is to recognize the meaning of the data. Once meaning is understood you automatically recognize all new data into same category. Look only at the data and not at column names, column names can be cryptic. Generate a context or meaning of the data by looking at the values present in the data. Use the destination columns as target column and identify how columns from file1 and file3 can be mapped to columns in destination model.   							
    Algorithm for Data Recognition and Categorization
    Please produce an output that matches this regex format:
    {regex_pattern}
    The single quotes must be typewriter-style or ASCII single quote (').

1. Data Type Identification:
   - Identify text vs numeric fields by looking at the data (ignore column headings).

2. Pattern Recognition:
   - Rows from file1 and file2 :
     -file1:
     {data1}
     -file2:
     {data2}
   - Look for recognizable patterns (names, numeric IDs, etc.) without relying on headers.

3. Contextual Analysis:
   - Use real content given above to infer meaning (e.g., numeric sequences as account/ID, text as names).

4. Mapping and Categorization:
   - Destination Format: {destination_columns}
   - Identify how columns in file1 and file2 map to destination by values, not headers.

5. Data Mapping:
    - Map the data from file1 and file2 to destination.
    - Use the values in the data to determine the mapping, not the headers.
    - Provide a clear explanation of the mapping and categorization process.
    -Make sure all the columns of destination are mapped to a column in file1 or file2,it can also be mapped to columns from both tables.

6. Data Categorization and Table Generation:
   - Provide a  table with columns: "Destination Column" , "File 1 Column" , "File 2 Column" , "Reasoning"
   -IMPORTANT: Return the table enclosed in triple single quotes on separate lines: 
     '''
      Destination Column,File 1 Column,File 2 Column,Reasoning
      value1,value2,value3,"value4"
     ...
     '''
     NO markdown or HTML formatting is needed.
     *Ensure that only the cells in "Reasoning" column are enclosed in double quotes.*
   - Summarize how each column in file1 and file2 corresponds to the destination column based on identified patterns.
"""

    chat_completion = client.chat.completions.create(
        model="deepseek-chat",  # DeepSeek model name
        messages=[
            {
                "role": "user",
                "content": prompt,
            }
        ],
    )
    result = chat_completion.choices[0].message.content
    formatted_result = result.strip()

    return {"result": formatted_result}

@app.post("/update-table/")
async def update_table(
    table: str = Form(...),
    suggestion: str = Form(...),
    file1: UploadFile = File(...),
    file2: UploadFile = File(...),
):
    # Read file1 and file2 content for additional context
    data1 = pd.read_csv(BytesIO(await file1.read())).head(5).to_csv(index=False)
    data2 = pd.read_csv(BytesIO(await file2.read())).head(5).to_csv(index=False)

    # Construct the update prompt with context
    update_prompt = f"""
    The following table was generated by the AI:
    {table}
    
    The user has suggested the following changes:
    {suggestion}
    
    To help you update the table, here are sample rows from the source files:
    
    File 1:
    {data1}
    
    File 2:
    {data2}
    
    - Provide a table with columns: "Destination Column", "File 1 Column", "File 2 Column", "Reasoning"
    - Ensure that only the cells in the "Reasoning" column are enclosed in double quotes.
    - All other cells should *not* have quotes.
    - Use a comma , as the delimiter.
    - Return the table enclosed in triple single quotes on separate lines:
      '''
      Destination Column,File 1 Column,File 2 Column,Reasoning
      First Name,name1,user1,"Represents first names from both files."
      Last Name,name2,user2,"Represents last names from both files."
      ...
      '''
    - NO markdown or HTML formatting is needed.
    Please produce an output that matches this regex format:
    {regex_pattern}
    The single quotes must be typewriter-style or ASCII single quote (').
    """

    # Query the LLM for the updated table
    chat_completion = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "user", "content": update_prompt}],
    )
    updated_result = chat_completion.choices[0].message.content.strip()

    return {"updated_table": updated_result}

@app.post("/generate-sql")
async def generate_sql_statements(
    file1_content: str = Form(...),
    file2_content: str = Form(...),
    destination_columns: str = Form(...),
    df_final: str = Form(...)
):
    prompt = f"""
You are an expert SQL developer. Based on the following information, generate SQL statements to create tables for file1, file2, and destination, including primary keys and necessary constraints. Handle cases where a destination column maps to multiple source columns. Include SQL constraints such as primary keys, foreign keys, and any necessary UNIQUE or NOT NULL constraints.

*File1 Sample Data:*
{file1_content}

*File2 Sample Data:*
{file2_content}

*Destination Columns:*
{destination_columns}

*Mapping Table (df_final):*
{df_final}

*Instructions:*
1. *CREATE TABLE Statements:*
   - Define tables for file1, file2, and destination based on the mapping.
   - Choose appropriate data types.
   - Set primary keys for each table.
   - Add foreign key constraints where necessary.

2. *INSERT INTO Statements:*
   - Generate INSERT statements to populate the destination table based on the mappings.
   - Handle cases where destination columns are derived from multiple source columns.

3. *Constraints:*
   - Add any additional constraints that ensure data integrity based on the mappings.

*Output Format:*
Provide all SQL statements enclosed within triple single quotes as shown below:
'''
-- Your SQL Statements Here
'''
Ensure no markdown or HTML formatting is included.
"""

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0  # For more deterministic output
        )
        sql_output = response.choices[0].message.content.strip()
        
        # Extract SQL statements enclosed in triple single quotes
        if "'''" in sql_output:
            sql_statements = sql_output.split("'''")[1]
        else:
            sql_statements = sql_output  # Fallback if not properly formatted
        
        return {"sql_statements": sql_statements}
    
    except Exception as e:
        return {"error": str(e)}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)